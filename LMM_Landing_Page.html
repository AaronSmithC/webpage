<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0"/>
  <title>Large Movement Model (LMM)</title>
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;800&display=swap" rel="stylesheet">
  <style>
    body {
      margin: 0;
      font-family: 'Inter', sans-serif;
      background-color: #f8f9fa;
      color: #333;
    }
    header {
      background-color: #001F3F;
      color: #fff;
      padding: 2.5rem 1.5rem;
      text-align: center;
    }
    header h1 {
      margin: 0;
      font-size: 2.7rem;
      letter-spacing: 0.03em;
    }
    header p {
      font-size: 1.1rem;
      opacity: 0.95;
      max-width: 760px;
      margin: 0.75rem auto 0;
    }
    main {
      max-width: 960px;
      margin: 2rem auto 3rem;
      padding: 0 1rem;
    }
    section {
      margin-bottom: 2.5rem;
    }
    section h2 {
      color: #001F3F;
      font-size: 1.8rem;
      margin-bottom: 0.75rem;
    }
    p {
      line-height: 1.6;
    }
    ul {
      padding-left: 1.2rem;
      list-style-type: disc;
      line-height: 1.6;
    }
    .pill-row {
      display: flex;
      flex-wrap: wrap;
      gap: 0.5rem;
      margin: 0.75rem 0 0;
    }
    .pill {
      background: #e3ecf8;
      color: #001F3F;
      padding: 0.3rem 0.75rem;
      border-radius: 999px;
      font-size: 0.85rem;
      font-weight: 600;
    }
    .highlight {
      font-weight: 600;
    }
    footer {
      text-align: center;
      padding: 2rem;
      font-size: 0.9rem;
      color: #666;
      background-color: #eaeaea;
    }
    a {
      color: #0074D9;
      text-decoration: none;
    }
    a:hover {
      text-decoration: underline;
    }
  </style>
</head>
<body>

<header>
  <h1>Large Movement Model (LMM)</h1>
  <p>
    A foundational AI model for motion — learning from sequences of bodies, hands, faces, objects, and vehicles the way language models learn from text.
  </p>
</header>

<section style="background:#f0f4ff; padding:1.25rem 1rem; text-align:center; border-bottom:1px solid #d6e2ff;">
  <p style="margin:0; font-size:1rem; color:#003366;">
    Want on-device motion extraction today? Try our companion app
    <strong>MovementModeler</strong> — turn iPhone video into clean BODY-25 stick-figure motion and JSON exports.
  </p>
  <a href="https://apps.apple.com/us/app/movementmodeler/id6755984861" 
     target="_blank" rel="noopener noreferrer" 
     style="display:inline-block; margin-top:0.7rem;">
    <img 
      src="https://developer.apple.com/assets/elements/badges/download-on-the-app-store.svg"
      alt="Download on the App Store"
      style="height:34px;">
  </a>
</section>

<main>
  <section>
    <h2>What is a Large Movement Model?</h2>
    <p>
      The <span class="highlight">Large Movement Model (LMM)</span> is a general-purpose AI system that treats
      <span class="highlight">motion itself</span> as the core data type. Instead of learning from words and sentences,
      LMM learns from <span class="highlight">sequences of movement</span>—joint positions, trajectories, interactions,
      and timing across people, tools, and vehicles.
    </p>
    <p>
      In the same way large language models model how text unfolds, LMM models how motion unfolds:
      recognizing activities, forecasting what comes next, and evaluating how plausible or “healthy”
      a movement is across many real-world settings.
    </p>
    <p>
      You can think of it as an <em>“LLM for motion”</em>—a shared model family that can be adapted to
      digital health, safety, sports, robotics, and other embodied AI domains.
    </p>
  </section>

  <section>
    <h2>Why Motion, Why Now?</h2>
    <p>
      Text, images, and audio already have mature foundation models. Motion does not.
      Today, most systems that reason about movement are narrow point solutions:
      one model for rehab, another for sports, another for driving or security.
    </p>
    <p>
      LMM is designed to treat motion as a first-class sequence domain, so that the same
      underlying representation and model family can support:
    </p>
    <ul>
      <li>Full-body human movement (gait, posture, activities)</li>
      <li>Fine movements of hands, fingers, and face</li>
      <li>Objects and tools (balls, bats, canes, industrial tools)</li>
      <li>Vehicles and macro-objects (cars, forklifts, delivery robots)</li>
      <li>Multi-agent and crowd dynamics (people and vehicles together)</li>
    </ul>
    <div class="pill-row">
      <span class="pill">Cross-domain by design</span>
      <span class="pill">Biomechanically and physically grounded</span>
      <span class="pill">Built for real environments, not lab demos</span>
    </div>
  </section>

  <section>
    <h2>Anchor Application Domains</h2>
    <p>
      We are actively developing and validating LMM across three initial verticals, with
      additional domains to follow:
    </p>
    <ul>
      <li>
        <strong>Digital Health &amp; Rehabilitation</strong> – Gait and balance analytics, detection of compensation and fall-risk signatures, and objective progress tracking from routine rehab sessions and home exercises.
      </li>
      <li>
        <strong>Security, Safety &amp; Industrial Monitoring</strong> – Motion-aware anomaly detection and behavior understanding for CCTV and workplace video, with a focus on privacy-preserving, pose-based analysis instead of identity-first tracking.
      </li>
      <li>
        <strong>Sports &amp; Performance Analytics</strong> – Biomechanics, fatigue and injury-risk indicators, and team-level motion patterns that shape spacing, effort, and game dynamics.
      </li>
    </ul>
    <p>
      The same underlying motion representation can also be extended to
      <em>collaborative robotics, AR/VR avatars, human–AI interaction, and other embodied AI use cases</em>
      as the ecosystem matures.
    </p>
  </section>

  <section>
    <h2>How LMM Works (High Level)</h2>
    <p>
      At a high level, LMM operates on normalized motion sequences derived from video and other sensors.
      A typical pipeline looks like:
    </p>
    <ul>
      <li><strong>1. Capture</strong> – Video or multi-camera streams from clinics, courts, gyms, or workplaces.</li>
      <li><strong>2. Pose &amp; Trajectory Extraction</strong> – Pose estimators and trackers produce keypoints for people, hands, faces, tools, and vehicles.</li>
      <li><strong>3. Normalization</strong> – Sequences are centered, scaled, re-timed, and tagged with metadata (domain, confidence, occlusion, temporal scale).</li>
      <li><strong>4. Motion Tokens</strong> – Frames, short windows, and/or learned motion primitives are turned into compact “tokens” analogous to words in language modeling.</li>
      <li><strong>5. LMM Inference</strong> – A transformer-style (and, in future phases, mixture-of-experts and diffusion) model reasons over these tokens to classify, forecast, and score motion.</li>
    </ul>
    <p>
      This architecture allows a single model family to support tasks such as activity recognition,
      short-term motion prediction, sequence completion, and motion-quality scoring.
    </p>
  </section>

  <section>
    <h2>How MovementModeler Fits In</h2>
    <p>
      <strong>MovementModeler</strong> is our on-device front door into the LMM ecosystem. It lets users:
    </p>
    <ul>
      <li>Convert iPhone video into stabilized stick-figure motion clips</li>
      <li>Export BODY-25 JSON for use in research and computer-vision pipelines</li>
      <li>Experiment with smoothing, tracking, and motion visualization without any server setup</li>
    </ul>
    <p>
      Under the hood, the same kinds of normalized skeleton data and export formats that MovementModeler
      produces are exactly what LMM consumes at larger scale. The app is a practical tool today and a
      data/interaction bridge into tomorrow’s motion foundation models.
    </p>
  </section>

  <section>
    <h2>Partnerships &amp; Pilots</h2>
    <p>
      LMM is being developed by <strong>LMM Technologies Inc.</strong>, a spin-out of
      <strong>Aegis Station Infrastructure LLC</strong>, as an early entrant in motion-centric
      foundation models. We are currently:
    </p>
    <ul>
      <li>Building and testing LMM prototypes on curated, de-identified motion datasets</li>
      <li>Collaborating with academic partners on model design, tokenization, and evaluation</li>
      <li>Scoping pilot projects with organizations in rehab, safety/security, and sports analytics</li>
    </ul>
    <p>
      If you are interested in research collaborations, pilot deployments, or future licensing and APIs,
      we’d be glad to talk.
    </p>
  </section>

  <section>
    <h2>Contact</h2>
    <p>
      For inquiries about partnerships, pilots, or staying informed as we share results:
      <br/>
      <a href="mailto:engage@aegisstation.com">engage@aegisstation.com</a>
    </p>
  </section>
</main>

<footer>
  &copy; 2025 LMM Technologies Inc. &amp; Aegis Station Infrastructure LLC.
  Large Movement Model (LMM)&trade;. All rights reserved.
</footer>

</body>
</html>